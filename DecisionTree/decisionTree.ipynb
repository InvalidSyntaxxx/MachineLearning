{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def dataTransform(path)-> None:  # 对离散数据值进行转换\n",
    "    data = pd.read_csv(path)\n",
    "    data = data[['色泽','根蒂','敲声','纹理','脐部','触感','密度','含糖率','好瓜']]\n",
    "    returndata = []\n",
    "    for datai in data.values:\n",
    "        temp =[]\n",
    "        for dataii in datai:\n",
    "            if dataii in ['青绿','蜷缩','清脆','清晰','平坦','硬滑','否']:\n",
    "                temp.append(0.0)\n",
    "            elif dataii in ['乌黑','稍蜷','浊响','稍糊','稍凹']:\n",
    "                temp.append(0.5)\n",
    "            elif isinstance(dataii,float) == True:\n",
    "                temp.append(dataii)\n",
    "            else:\n",
    "                temp.append(1.0)\n",
    "        returndata.append(temp)\n",
    "    return returndata\n",
    "# np.array(dataTransform())\n",
    "\n",
    "def sigmoid(z):\n",
    "\treturn 1.0 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/watermelon.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20772/3102560912.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'datasets/watermelon.csv'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 读取数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mAttributes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m        \u001b[1;31m#所有属性的名称\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#print(Attributes)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/watermelon.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams[u'font.sans-serif'] = ['simhei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    " \n",
    " \n",
    "dataset = pd.read_csv('datasets/watermelon.csv')  # 读取数据\n",
    "Attributes = dataset.columns        #所有属性的名称\n",
    "#print(Attributes)\n",
    "m,n = np.shape(dataset)   # 得到数据集大小\n",
    "dataset = np.mat(dataset)\n",
    "for i in range(m):      # 将标签替换成 好瓜 和 坏瓜\n",
    "    if dataset[i,n-1]=='是': dataset[i,n-1] = '好瓜'\n",
    "    else : dataset[i,n-1] = '坏瓜'\n",
    "attributeList = []       # 属性列表，每一个属性的取值，列表中元素是集合\n",
    "for i in range(n):\n",
    "    curSet = set()      # 使用集合是利用了集合里面元素不可重复的特性，从而提取出了每个属性的取值\n",
    "    for j in range(m):\n",
    "        curSet.add(dataset[j,i])\n",
    "    attributeList.append(curSet)\n",
    "#print(attributeList)\n",
    "D = np.arange(0,m,1)     # 表示每一个样本编号\n",
    "A = list(np.ones(n))    # 表示每一个属性是否被使用，使用过了标为 -1\n",
    "A[-1] = -1              # 将数据里面的标签和编号列标记为 -1\n",
    "A[0] = -1\n",
    "#print(A)\n",
    "#print(D)\n",
    "EPS = 0.000001\n",
    " \n",
    " \n",
    "class Node(object):            # 创建一个类，用来表示节点的信息\n",
    "    def __init__(self,title):\n",
    "        self.title = title     # 上一级指向该节点的线上的标记文字\n",
    "        self.v = 1              # 节点的信息标记\n",
    "        self.children = []     # 节点的孩子列表\n",
    "        self.deep = 0         # 节点深度\n",
    "        self.ID = -1         # 节点编号\n",
    " \n",
    "def isSameY(D):                  # 判断所有样本是否属于同一类\n",
    "    curY = dataset[D[0],n-1]\n",
    "    for i in range(1,len(D)):\n",
    "        if dataset[D[i],n-1] != curY:\n",
    "            return  False\n",
    "    return True\n",
    " \n",
    "def isBlankA(A):              # 判断 A 是否是空，是空则返回true\n",
    "    for i in range(n):\n",
    "        if A[i]>0: return False\n",
    "    return True\n",
    " \n",
    "def isSameAinD(D,A):       # 判断在D中，是否所有的未使用过的样本属性均相同\n",
    "    for i in range(n):\n",
    "        if A[i]>0:\n",
    "            for j in range(1,len(D)):\n",
    "                if not isSameValue(dataset[D[0],i],dataset[D[j],i],EPS):\n",
    "                    return False\n",
    "    return True\n",
    " \n",
    "def isSameValue(v1,v2,EPS):            # 判断v1、v2 是否相等\n",
    "    if type(v1)==type(dataset[0,8]):\n",
    "        return abs(v1-v2)<EPS\n",
    "    else: return  v1==v2\n",
    " \n",
    "def mostCommonY(D):             # 寻找D中样本数最多的类别\n",
    "    res = dataset[D[0],n-1]     # D中第一个样本标签\n",
    "    maxC = 1\n",
    "    count = {}\n",
    "    count[res] = 1              # 该标签数量记为1\n",
    "    for i in range(1,len(D)):\n",
    "        curV = dataset[D[i],n-1]   # 得到D中第i+1个样本的标签\n",
    "        if curV not in count:      # 若之前不存在这个标签\n",
    "            count[curV] = 1        # 则该标签数量记为1\n",
    "        else:count[curV] += 1      # 否则 ，该标签对应的数量加一\n",
    "        if count[curV]>maxC:       # maxC始终存贮最多标签对应的样本数量\n",
    "            maxC = count[curV]     # res 存贮当前样本数最多的标签类型\n",
    "            res = curV\n",
    "    return res             # 返回的是样本数最多的标签的类型\n",
    " \n",
    "def entropyD(D):       # 参数D中所存的样本的交叉熵\n",
    "    types = []         # 存贮类别标签\n",
    "    count = {}         # 存贮每个类别对应的样本数量\n",
    "    for i in range(len(D)):           # 统计D中存在的每个类型的样本数量\n",
    "        curY = dataset[D[i],n-1]\n",
    "        if curY not in count:\n",
    "            count[curY] = 1\n",
    "            types.append(curY)\n",
    "        else:\n",
    "            count[curY] += 1\n",
    "    ans = 0\n",
    "    total = len(D)                # D中样本总数量\n",
    "    for i in range(len(types)):   # 计算交叉熵\n",
    "        ans -= count[types[i]]/total*math.log2(count[types[i]]/total)\n",
    "    return ans\n",
    " \n",
    "def gain(D,p):        # 属性 p 上的信息增益\n",
    "    if type(dataset[0,p])== type(dataset[0,8]):   # 判断若是连续属性，则调用另一个函数\n",
    "        res,divideV = gainFloat(D,p)\n",
    "    else:\n",
    "        types = []\n",
    "        count = {}\n",
    "        for i in range(len(D)):  # 得到每一个属性取值上的样本编号\n",
    "            a = dataset[D[i],p]\n",
    "            if a not in count:\n",
    "                count[a] = [D[i]]\n",
    "                types.append(a)\n",
    "            else:\n",
    "                count[a].append(D[i])\n",
    "        res = entropyD(D)              # D的交叉熵\n",
    "        total = len(D)\n",
    "        for i in range(len(types)):    # 计算出每一个属性取值分支上的交叉熵，再计算出信息增益\n",
    "            res -= len(count[types[i]])/total*entropyD(count[types[i]])\n",
    "        divideV = -1000              # 这个只是随便给的一个值，没有实际意义\n",
    "    return res,divideV\n",
    " \n",
    "def gainFloat(D,p):            # 获得在连续属性上的最大信息增益及对应的划分点\n",
    "    a = []\n",
    "    for i in range(len(D)):    # 得到在该属性上的所有取值\n",
    "        a.append(dataset[D[i],p])\n",
    "    a.sort()     # 排序\n",
    "    T = []\n",
    "    for i in range(len(a)-1):       # 计算每一个划分点\n",
    "        T.append((a[i]+a[i+1])/2)\n",
    "    res = entropyD(D)               # D的交叉熵\n",
    "    ans = 0\n",
    "    divideV = T[0]\n",
    "    for i in range(len(T)):         # 循环根据每一个分割点进行划分\n",
    "        left = []\n",
    "        right = []\n",
    "        for j in range(len(D)):     # 根据特定分割点将样本分成两部分\n",
    "            if (dataset[D[j],p]<=T[i]):\n",
    "                left.append(D[j])\n",
    "            else:right.append(D[j])\n",
    "        temp = res-entropyD(left)-entropyD(right)    # 计算特定分割点下的信息增益\n",
    "        if temp>ans:\n",
    "            divideV = T[i]     # 始终存贮产生最大信息增益的分割点\n",
    "            ans = temp         # 存贮最大的信息增益\n",
    "    return ans,divideV\n",
    " \n",
    "def treeGenerate(D,A,title):\n",
    "    node = Node(title)\n",
    "    if isSameY(D):             # D中所有样本是否属于同一类\n",
    "        node.v = dataset[D[0],n-1]\n",
    "        return node\n",
    " \n",
    "    # 是否所有属性全部使用过  或者  D中所有样本的未使用的属性均相同\n",
    "    if isBlankA(A) or isSameAinD(D,A):\n",
    "        node.v = mostCommonY(D)  # 此时类别标记为样本数最多的类别（暗含可以处理存在异常样本的情况）\n",
    "        return node              # 否则所有样本的类别应该一致\n",
    " \n",
    "    entropy = 0\n",
    "    floatV = 0\n",
    "    p = 0\n",
    "    for i in range(len(A)):      # 循环遍历A,找可以获得最大信息增益的属性\n",
    "        if(A[i]>0):\n",
    "            curEntropy,divideV = gain(D,i)\n",
    "            if curEntropy > entropy:\n",
    "                p = i                     # 存贮属性编号\n",
    "                entropy = curEntropy\n",
    "                floatV = divideV\n",
    " \n",
    "    if isSameValue(-1000,floatV,EPS):   # 说明是离散属性\n",
    "        node.v = Attributes[p]+\"=?\"     # 节点信息\n",
    "        curSet = attributeList[p]       # 该属性的所有取值\n",
    "        for i in curSet:\n",
    "            Dv = []\n",
    "            for j in range(len(D)):     # 获得该属性取某一个值时对应的样本标号\n",
    "                if dataset[D[j],p]==i:\n",
    "                    Dv.append(D[j])\n",
    " \n",
    "            # 若该属性取值对应没有符合的样本，则将该分支作为叶子，类别是D中样本数最多的类别\n",
    "            # 其实就是处理在没有对应的样本情况下的问题。那就取最大可能性的一类。\n",
    "            if Dv==[]:\n",
    "                nextNode = Node(i)\n",
    "                nextNode.v = mostCommonY(D)\n",
    "                node.children.append(nextNode)\n",
    "            else:     # 若存在对应的样本，则递归继续生成该节点下的子树\n",
    "                newA = copy.deepcopy(A)    # 注意是深度复制，否则会改变A中的值\n",
    "                newA[p]=-1\n",
    "                node.children.append(treeGenerate(Dv,newA,i))\n",
    "    else:   # 若对应的是连续的属性\n",
    "        Dleft = []\n",
    "        Dright = []\n",
    "        node.v = Attributes[p]+\"<=\"+str(floatV)+\"?\"     # 节点信息\n",
    "        for i in range(len(D)):       # 根据划分点将样本分成左右两部分\n",
    "            if dataset[D[i],p]<=floatV: Dleft.append(D[i])\n",
    "            else: Dright.append(D[i])\n",
    "        node.children.append(treeGenerate(Dleft,A[:],\"是\"))    # 左边递归生成子树，是 yes 分支\n",
    "        node.children.append(treeGenerate(Dright,A[:],\"否\"))    # 同上。 注意，在此时没有将对应的A中值变成 -1\n",
    "    return node                                                # 因为连续属性可以使用多次进行划分\n",
    " \n",
    "def countLeaf(root,deep):\n",
    "    root.deep = deep\n",
    "    res = 0\n",
    "    if root.v=='好瓜' or root.v=='坏瓜':   # 说明此时已经是叶子节点了，所以直接返回\n",
    "        res += 1\n",
    "        return res,deep\n",
    "    curdeep = deep             # 记录当前深度\n",
    "    for i in root.children:    # 得到子树中的深度和叶子节点的个数\n",
    "        a,b = countLeaf(i,deep+1)\n",
    "        res += a\n",
    "        if b>curdeep: curdeep = b\n",
    "    return res,curdeep\n",
    " \n",
    "def giveLeafID(root,ID):         # 给叶子节点编号\n",
    "    if root.v=='好瓜' or root.v=='坏瓜':\n",
    "        root.ID = ID\n",
    "        ID += 1\n",
    "        return ID\n",
    "    for i in root.children:\n",
    "        ID = giveLeafID(i,ID)\n",
    "    return ID\n",
    " \n",
    "def plotNode(nodeTxt,centerPt,parentPt,nodeType):     # 绘制节点\n",
    "    plt.annotate(nodeTxt,xy = parentPt,xycoords='axes fraction',xytext=centerPt,\n",
    "                 textcoords='axes fraction',va=\"center\",ha=\"center\",bbox=nodeType,\n",
    "                 arrowprops=arrow_args)\n",
    " \n",
    "def dfsPlot(root):\n",
    "    if root.ID==-1:          # 说明根节点不是叶子节点\n",
    "        childrenPx = []\n",
    "        meanPx = 0\n",
    "        for i in root.children:\n",
    "            cur = dfsPlot(i)\n",
    "            meanPx += cur\n",
    "            childrenPx.append(cur)\n",
    "        meanPx = meanPx/len(root.children)\n",
    "        c = 0\n",
    "        for i in root.children:\n",
    "            nodetype = leafNode\n",
    "            if i.ID<0: nodetype=decisionNode\n",
    "            plotNode(i.v,(childrenPx[c],0.9-i.deep*0.8/deep),(meanPx,0.9-root.deep*0.8/deep),nodetype)\n",
    "            plt.text((childrenPx[c]+meanPx)/2,(0.9-i.deep*0.8/deep+0.9-root.deep*0.8/deep)/2,i.title)\n",
    "            c += 1\n",
    "        return meanPx\n",
    "    else:\n",
    "        return 0.1+root.ID*0.8/(cnt-1)\n",
    " \n",
    " \n",
    " \n",
    "myDecisionTreeRoot = treeGenerate(D,A,\"root\")        # 生成决策树\n",
    "cnt,deep = countLeaf(myDecisionTreeRoot,0)     # 得到树的深度和叶子节点的个数\n",
    "giveLeafID(myDecisionTreeRoot,0)\n",
    "# 绘制决策树\n",
    "decisionNode = dict(boxstyle = \"sawtooth\",fc = \"0.9\",color='blue')\n",
    "leafNode = dict(boxstyle = \"round4\",fc=\"0.9\",color='red')\n",
    "arrow_args = dict(arrowstyle = \"<-\",color='green')\n",
    "fig = plt.figure(1,facecolor='white')\n",
    "rootX = dfsPlot(myDecisionTreeRoot)\n",
    "plotNode(myDecisionTreeRoot.v,(rootX,0.9),(rootX,0.9),decisionNode)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
